{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Count_using_data_frames.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJ6BwGQEPq6roGA/3tN5gb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujana-7/pySpark/blob/main/Word_Count_using_data_frames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeVsp1gUpPgj"
      },
      "source": [
        "  pySpark Word Count program:\n",
        "  Steps followed-\n",
        "1.Configure Google Colab to run pySpark\n",
        "2.Upload data file from drive to colab\n",
        "3.Convert all uppercase words to lower case and remove all punctuation.\n",
        "4.Get the word count.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfPyAxGZFvNb"
      },
      "source": [
        "\n",
        "#Configuring google colab with pySpark \n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNchf0o6GaLU"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04NV7fLZGfj7"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzXzZDTxG2uE"
      },
      "source": [
        "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
        "#Function to clean an input text file with punctuations and convert to lowercase \n",
        "def removePunctuation(column):\n",
        "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
        "\n",
        "    Note:\n",
        "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
        "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
        "        punctuation is removed.\n",
        "\n",
        "    Args:\n",
        "        column (Column): A Column containing a sentence.\n",
        "\n",
        "    Returns:\n",
        "        Column: A Column named 'sentence' with clean-up operations applied.\n",
        "    \"\"\"\n",
        "    return trim(lower(regexp_replace(column, '([^\\s\\w_]|_)+', ''))).alias('sentence')\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfxo9fRzHDQc"
      },
      "source": [
        "# code to test function\n",
        "sentenceDF = sqlContext.createDataFrame([('Hi,  This',),\n",
        "                                         (' Is just to test above function!',),\n",
        "                                         (' *  ---Check if these punctuations are removed---  * ',)], ['sentence'])\n",
        "  # dispaly first original sentence\n",
        "sentenceDF.show(truncate=False)\n",
        "  # then sentence with punctuation removed\n",
        "(sentenceDF\n",
        " .select(removePunctuation(col('sentence')))\n",
        " .show(truncate=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA1imMAGIsNp"
      },
      "source": [
        "#IMPORT FILES FROM DRIVE INTO GOOGLE-COLAB:\n",
        "\n",
        "#STEP-1: Import Libraries\n",
        "\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#STEP-2: Autheticate E-Mail ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aIPwwj1JHoN"
      },
      "source": [
        "#2.1 Get the file\n",
        "downloaded = drive.CreateFile({'id':'17neskYlVTp7q4_T4_TpM9GM6VoRR2pp7'})\n",
        " # replace the id with id of file you want to access , which is found on google drive.\n",
        "#On the file on your drive, right click on the file and click get shareable link , copy paste\n",
        "#Remove the part of the link other than ID\n",
        "downloaded.GetContentFile('Word_Count_Data.txt') \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhtxFDv6Ig9w"
      },
      "source": [
        "fileName = \"Word_Count_Data.txt\"\n",
        "\n",
        "DF = sqlContext.read.text(fileName).select(removePunctuation(col('value')))\n",
        "DF.show(15, truncate=False)\n",
        "DF.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoUsyhSXRB3e"
      },
      "source": [
        "from pyspark.sql.functions import split, explode\n",
        "\n",
        "DF2 = (DF .select(split(DF.sentence, '\\s+').alias('split')))\n",
        "DF3 = (DF2.select(explode(DF2.split).alias('word')))\n",
        "\n",
        "DF3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqhctbffTGJr"
      },
      "source": [
        "wordListDF= DF3.groupBy('word').count()\n",
        "\n",
        "wordListDF.show(100)\n",
        "wordListDF.count()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}